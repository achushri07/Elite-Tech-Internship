# 🧠 Task 1 – Big Data Analysis

This project is part of the Elite Tech Intern **Data Analytics Internship**. The goal of this task is to perform data analysis on a large dataset using scalable tools suitable for big data processing. I used **Dask**, a parallel computing library in Python, to work efficiently with large data.

---

## 📁 Dataset

- **Name**: Supply Chain Big Data (synthetically generated)
- **Size**: 100,000+ rows, 15 columns
- **Format**: CSV
- **Features include**: Order details, customer segments, regions, pricing, shipping details, priorities, etc.

---

## 🛠️ Tools & Technologies

- Python 🐍  
- [Dask](https://www.dask.org/) – for scalable dataframe operations  
- Jupyter Notebook – for writing and executing code  
- GitHub – for version control and project delivery

---

## 🔍 Key Analyses Performed

- Total revenue by product category  
- Average shipping cost by region  
- Most popular shipping modes  
- Revenue breakdown by customer segment  
- Count and revenue of high-priority orders  
- Top 5 cities by order volume  
- Correlation between quantity, price, and shipping cost

---

## 📌 Results

Some insights derived:
- **Electronics** generated the highest total revenue.
- The **South region** had the highest average shipping cost.
- **Standard Class** was the most frequently used shipping mode.
- **Corporate customers** contributed to the largest share of revenue.

---

## 📂 Files in This Folder

- `supply_chain_analysis.ipynb` – Jupyter Notebook with code and analysis  
- `supply_chain_data.csv` – The dataset used  
- `README.md` – This file  

---

## ✅ Status

✅ Task Completed and ready for review/feedback.

