# ğŸ§  Task 1 â€“ Big Data Analysis

This project is part of the Elite Tech Intern **Data Analytics Internship**. The goal of this task is to perform data analysis on a large dataset using scalable tools suitable for big data processing. I used **Dask**, a parallel computing library in Python, to work efficiently with large data.

---

## ğŸ“ Dataset

- **Name**: Supply Chain Big Data (synthetically generated)
- **Size**: 100,000+ rows, 15 columns
- **Format**: CSV
- **Features include**: Order details, customer segments, regions, pricing, shipping details, priorities, etc.

---

## ğŸ› ï¸ Tools & Technologies

- Python ğŸ  
- [Dask](https://www.dask.org/) â€“ for scalable dataframe operations  
- Jupyter Notebook â€“ for writing and executing code  
- GitHub â€“ for version control and project delivery

---

## ğŸ” Key Analyses Performed

- Total revenue by product category  
- Average shipping cost by region  
- Most popular shipping modes  
- Revenue breakdown by customer segment  
- Count and revenue of high-priority orders  
- Top 5 cities by order volume  
- Correlation between quantity, price, and shipping cost

---

## ğŸ“Œ Results

Some insights derived:
- **Electronics** generated the highest total revenue.
- The **South region** had the highest average shipping cost.
- **Standard Class** was the most frequently used shipping mode.
- **Corporate customers** contributed to the largest share of revenue.

---

## ğŸ“‚ Files in This Folder

- `supply_chain_analysis.ipynb` â€“ Jupyter Notebook with code and analysis  
- `supply_chain_data.csv` â€“ The dataset used  
- `README.md` â€“ This file  

---

## âœ… Status

âœ… Task Completed and ready for review/feedback.

